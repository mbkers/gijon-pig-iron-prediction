{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model results analysis\n",
    "\n",
    "This notebook analyses the GPR model results and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src import utils\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test results\n",
    "predictions = pd.read_csv('../results/csv/test_predictions.csv', parse_dates=['date'])\n",
    "metrics = pd.read_csv('../results/csv/test_metrics.csv')\n",
    "feature_importance = pd.read_csv('../results/csv/feature_importance.csv')\n",
    "\n",
    "print(\"Model Performance:\")\n",
    "for col in metrics.columns:\n",
    "    print(f\"{col}: {metrics[col].values[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prediction analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series plot with uncertainty\n",
    "fig = utils.plot_predictions(\n",
    "        predictions['date'],\n",
    "        predictions['actual'],\n",
    "        predictions['predicted'],\n",
    "        predictions['std'],\n",
    "        title=\"Test Set Predictions\",\n",
    "        prediction_type=\"test\",\n",
    "    )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual analysis\n",
    "residuals = predictions['actual'] - predictions['predicted']\n",
    "standardised_residuals = residuals / predictions['std']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Residuals over time\n",
    "axes[0, 0].plot(predictions['date'], residuals, 'o-')\n",
    "axes[0, 0].axhline(y=0, color='r', linestyle='--')\n",
    "axes[0, 0].set_title('Residuals over Time')\n",
    "axes[0, 0].set_ylabel('Residual')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Residual histogram\n",
    "axes[0, 1].hist(residuals, bins=10, edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].set_title('Residual Distribution')\n",
    "axes[0, 1].set_xlabel('Residual')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "\n",
    "# Q-Q plot\n",
    "from scipy import stats\n",
    "stats.probplot(standardised_residuals, dist=\"norm\", plot=axes[1, 0])\n",
    "axes[1, 0].set_title('Q-Q Plot (Standardised Residuals)')\n",
    "\n",
    "# Uncertainty calibration\n",
    "in_interval = ((predictions['actual'] >= predictions['lower_95']) &\n",
    "               (predictions['actual'] <= predictions['upper_95']))\n",
    "coverage = in_interval.mean()\n",
    "\n",
    "axes[1, 1].scatter(predictions['std'], np.abs(residuals), alpha=0.6)\n",
    "axes[1, 1].plot([0, predictions['std'].max()],\n",
    "                [0, 1.96 * predictions['std'].max()],\n",
    "                'r--', label='Expected (95%)')\n",
    "axes[1, 1].set_xlabel('Predicted Std Dev')\n",
    "axes[1, 1].set_ylabel('|Residual|')\n",
    "axes[1, 1].set_title(f'Uncertainty Calibration (Coverage: {coverage:.1%})')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load modeling data to get feature correlations\n",
    "modeling_data = pd.read_csv('../results/csv/modeling_data.csv', index_col=0)\n",
    "\n",
    "# Calculate correlations for selected features\n",
    "selected_features = feature_importance['feature'].tolist()\n",
    "correlations = modeling_data[selected_features].corrwith(modeling_data['Value'])\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "y_pos = np.arange(len(selected_features))\n",
    "ax.barh(y_pos, correlations.abs().values)\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(selected_features)\n",
    "ax.set_xlabel('|Correlation with Production|')\n",
    "ax.set_title('Selected Feature Importance')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 5 features:\")\n",
    "for feat, corr in correlations.abs().nlargest(5).items():\n",
    "    print(f\"  {feat}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse prediction errors\n",
    "predictions['error'] = predictions['actual'] - predictions['predicted']\n",
    "predictions['pct_error'] = 100 * predictions['error'] / predictions['actual']\n",
    "\n",
    "# Identify worst predictions\n",
    "worst_predictions = pd.concat([\n",
    "    predictions.nlargest(3, 'error'),\n",
    "    predictions.nsmallest(3, 'error')\n",
    "])\n",
    "\n",
    "print(\"Worst predictions:\")\n",
    "print(worst_predictions[['date', 'actual', 'predicted', 'error', 'pct_error']].round(1))\n",
    "\n",
    "# Monthly error pattern\n",
    "predictions['month'] = predictions['date'].dt.month\n",
    "monthly_error = predictions.groupby('month')['pct_error'].agg(['mean', 'std'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "monthly_error['mean'].plot(kind='bar', yerr=monthly_error['std'],\n",
    "                          capsize=5, ax=ax)\n",
    "ax.axhline(y=0, color='r', linestyle='--')\n",
    "ax.set_xlabel('Month')\n",
    "ax.set_ylabel('Mean Percentage Error')\n",
    "ax.set_title('Prediction Error by Month')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Leave-One-Out Cross-Validation (LOO-CV) analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LOO-CV results\n",
    "loo_predictions = pd.read_csv('../results/csv/loo_predictions.csv', parse_dates=['date'])\n",
    "loo_metrics = pd.read_csv('../results/csv/loo_metrics.csv')\n",
    "\n",
    "print(\"LOO-CV Performance:\")\n",
    "for col in loo_metrics.columns:\n",
    "    print(f\"{col}: {loo_metrics[col].values[0]:.3f}\")\n",
    "\n",
    "print(\"\\nTest vs LOO-CV Comparison:\")\n",
    "for col in metrics.columns:\n",
    "    test_val = metrics[col].values[0]\n",
    "    loo_val = loo_metrics[col].values[0]\n",
    "    diff = loo_val - test_val\n",
    "    print(f\"{col}: Test={test_val:.3f}, LOO={loo_val:.3f}, Diff={diff:+.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series plot with uncertainty\n",
    "fig = utils.plot_predictions(\n",
    "        loo_predictions['date'],\n",
    "        loo_predictions['actual'],\n",
    "        loo_predictions['predicted'],\n",
    "        loo_predictions['std'],\n",
    "        title=\"LOO-CV Predictions\",\n",
    "        prediction_type=\"loo\",\n",
    "    )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOO-CV residual analysis\n",
    "loo_residuals = loo_predictions['actual'] - loo_predictions['predicted']\n",
    "loo_standardised_residuals = loo_residuals / loo_predictions['std']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# LOO residuals over time\n",
    "axes[0].plot(loo_predictions['date'], loo_residuals, 'o-', color='red', alpha=0.6)\n",
    "axes[0].axhline(y=0, color='k', linestyle='--')\n",
    "axes[0].set_title('LOO-CV Residuals over Time')\n",
    "axes[0].set_ylabel('Residual')\n",
    "axes[0].set_xlabel('Date')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# LOO-CV uncertainty calibration\n",
    "loo_in_interval = ((loo_predictions['actual'] >= loo_predictions['lower_95']) &\n",
    "                   (loo_predictions['actual'] <= loo_predictions['upper_95']))\n",
    "loo_coverage = loo_in_interval.mean()\n",
    "\n",
    "axes[1].scatter(loo_predictions['std'], np.abs(loo_residuals), alpha=0.6, color='red')\n",
    "axes[1].plot([0, loo_predictions['std'].max()],\n",
    "            [0, 1.96 * loo_predictions['std'].max()],\n",
    "            'k--', label='Expected (95%)')\n",
    "axes[1].set_xlabel('Predicted Std Dev')\n",
    "axes[1].set_ylabel('|Residual|')\n",
    "axes[1].set_title(f'LOO-CV Uncertainty Calibration (Coverage: {loo_coverage:.1%})')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare test vs LOO-CV statistics\n",
    "print(f\"\\nResidual Statistics Comparison:\")\n",
    "print(f\"Test Set - Mean: {residuals.mean():.2f}, Std: {residuals.std():.2f}\")\n",
    "print(f\"LOO-CV   - Mean: {loo_residuals.mean():.2f}, Std: {loo_residuals.std():.2f}\")\n",
    "print(f\"\\nCoverage Comparison:\")\n",
    "print(f\"Test Set: {coverage:.1%}\")\n",
    "print(f\"LOO-CV:   {loo_coverage:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Test vs LOO-CV predictions\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Test set actual vs predicted\n",
    "ax1.scatter(predictions['actual'], predictions['predicted'],\n",
    "           alpha=0.6, s=50, label='Test Set')\n",
    "min_val = min(predictions['actual'].min(), predictions['predicted'].min())\n",
    "max_val = max(predictions['actual'].max(), predictions['predicted'].max())\n",
    "ax1.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.5)\n",
    "ax1.set_xlabel('Actual Production')\n",
    "ax1.set_ylabel('Predicted Production')\n",
    "ax1.set_title(f'Test Set (R² = {metrics[\"r2\"].values[0]:.3f})')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# LOO-CV actual vs predicted\n",
    "ax2.scatter(loo_predictions['actual'], loo_predictions['predicted'],\n",
    "           alpha=0.6, s=50, color='red', label='LOO-CV')\n",
    "min_val = min(loo_predictions['actual'].min(), loo_predictions['predicted'].min())\n",
    "max_val = max(loo_predictions['actual'].max(), loo_predictions['predicted'].max())\n",
    "ax2.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.5)\n",
    "ax2.set_xlabel('Actual Production')\n",
    "ax2.set_ylabel('Predicted Production')\n",
    "ax2.set_title(f'LOO-CV (R² = {loo_metrics[\"r2\"].values[0]:.3f})')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key findings:\n",
    "1. GPR achieves modest predictive performance\n",
    "2. Uncertainty estimates are well-calibrated (95% coverage)\n",
    "3. Errors are relatively unbiased across months\n",
    "4. Thermal percentiles (p95, p99) are important features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
